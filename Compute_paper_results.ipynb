{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57cea4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "import time\n",
    "from scipy import special\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a54f2f-3b14-48e4-b3ca-0f480ac64d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multisoc.infer import data_loader\n",
    "from multisoc.infer import aux_functions\n",
    "from multisoc.infer import inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a76be9",
   "metadata": {},
   "source": [
    "# Full computation of AddHealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b9dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_comms = [48, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc99a49",
   "metadata": {},
   "source": [
    "### Dictionary to store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c1200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_list = ['grade','race','sex']\n",
    "num_dimensions = len(dimensions_list)\n",
    "sex_list = [\"Female\",\"Male\"]\n",
    "race_list = [\"White\",\"Black\",\"Hispanic\",\"Asian\",\"Mixed/other\"]\n",
    "grade_list = [\"7th\",\"8th\",\"9th\",\"10th\",\"11th\",\"12th\"]\n",
    "all_attributes_dict = {\n",
    "    \"grade\":grade_list,\n",
    "    \"race\":race_list,\n",
    "    \"sex\":sex_list\n",
    "}\n",
    "\n",
    "multidim_groups = list(itertools.product(*[all_attributes_dict[d] for d in dimensions_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f76f0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_functions = [\"and\",\n",
    "                         \"or\",\n",
    "                         \"mean\"\n",
    "                        ]\n",
    "preference_structures = [\"multi-1d\",\"1d-full\",\"1d-simple\"]\n",
    "\n",
    "aggr_pref_combinations = [\n",
    "    \"multi-full\",\n",
    "    \"and_1d-simple\",\n",
    "    \"or_1d-simple\",\n",
    "    \"mean_1d-simple\",\n",
    "    \"and_multi-1d\",\n",
    "    \"and_1d-full\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5910699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_results_dictionary(\n",
    "    all_attributes_dict,\n",
    "    aggr_pref_combinations\n",
    "    ):\n",
    "    results_dictionary = {\n",
    "        \"school\":[], ## School ID\n",
    "        \"N\":[] ## Total size\n",
    "    }\n",
    "\n",
    "    ## Sizes of one-dimensional groups\n",
    "    results_dictionary.update(\n",
    "        {\"N_\"+i:[] for i in grade_list+race_list+sex_list}\n",
    "    )\n",
    "\n",
    "    ## Sizes of multidimensional groups\n",
    "    multidim_groups = list(itertools.product(*all_attributes_dict.values()))\n",
    "    results_dictionary.update(\n",
    "        {\"N_\"+\"|\".join(i):[] for i in multidim_groups}\n",
    "    )\n",
    "\n",
    "    ## Estimated H for each pair of multidimensional groups\n",
    "    multidim_pairs = list(itertools.product(multidim_groups,multidim_groups))\n",
    "    for aggr_pref in aggr_pref_combinations:\n",
    "        results_dictionary.update(\n",
    "            {\"H_\" + aggr_pref + \"_\" + \"|\".join(i[0]) + \"-\" + \"|\".join(i[1]):[] for i in multidim_pairs}\n",
    "        )\n",
    "\n",
    "\n",
    "    ## Estimated h for each pair of one-dimensional groups\n",
    "    ## Also MRQAP p-values\n",
    "    ## Also MRQAP mean\n",
    "    ## Also MRQAP std\n",
    "    for aggr_pref in aggr_pref_combinations:\n",
    "\n",
    "        if \"1d-simple\" in aggr_pref:\n",
    "            for d, attr_list_d in all_attributes_dict.items():\n",
    "                onedim_pairs_d = list(itertools.product(attr_list_d,attr_list_d))\n",
    "                results_dictionary.update(\n",
    "                    {\"h_\" + aggr_pref + \"_\" + d + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "                )\n",
    "                results_dictionary.update(\n",
    "                    {\"h_norm_\" + aggr_pref + \"_\" + d + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "                )\n",
    "                \n",
    "                if aggr_pref == \"and_1d-simple\":\n",
    "                    results_dictionary.update(\n",
    "                        {\"MRQAP_pval1s_h_\" + aggr_pref + \"_\" + d + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "                    )\n",
    "                    results_dictionary.update(\n",
    "                        {\"MRQAP_pval2s_h_\" + aggr_pref + \"_\" + d + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "                    )\n",
    "                    results_dictionary.update(\n",
    "                        {\"MRQAP_av_h_\" + aggr_pref + \"_\" + d + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "                    )\n",
    "                    results_dictionary.update(\n",
    "                        {\"MRQAP_std_h_\" + aggr_pref + \"_\" + d + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "                    )\n",
    "                    results_dictionary.update(\n",
    "                        {\"MRQAP_av_h_norm_\" + aggr_pref + \"_\" + d + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "                    )\n",
    "                    results_dictionary.update(\n",
    "                        {\"MRQAP_std_h_norm_\" + aggr_pref + \"_\" + d + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "                    )\n",
    "\n",
    "        elif \"1d-full\" in aggr_pref:\n",
    "            onedim_pairs_d = list(itertools.product(itertools.chain.from_iterable(all_attributes_dict.values()),\n",
    "                                               itertools.chain.from_iterable(all_attributes_dict.values())\n",
    "                                              ))\n",
    "            results_dictionary.update(\n",
    "                {\"h_\" + aggr_pref + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "            )\n",
    "#             results_dictionary.update(\n",
    "#                 {\"MRQAP_pval_\" + aggr_pref + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "#             )\n",
    "#             results_dictionary.update(\n",
    "#                 {\"MRQAP_av_\" + aggr_pref + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "#             )\n",
    "#             results_dictionary.update(\n",
    "#                 {\"MRQAP_std_\" + aggr_pref + \"_\" + \"-\".join(i):[] for i in onedim_pairs_d}\n",
    "#             )\n",
    "\n",
    "        elif \"multi-1d\" in aggr_pref:\n",
    "            for multidim_group_i in multidim_groups:\n",
    "                onedim_groups_all = itertools.chain.from_iterable(all_attributes_dict.values())\n",
    "                results_dictionary.update(\n",
    "                    {\"h_\" + aggr_pref + \"_\" + \"|\".join(multidim_group_i) + \"-\" + i:[] for i in onedim_groups_all}\n",
    "                )\n",
    "#                 results_dictionary.update(\n",
    "#                     {\"MRQAP_pval_\" + aggr_pref + \"_\" + \"|\".join(multidim_group_i) + \"-\" + i:[] for i in onedim_groups_all}\n",
    "#                 )\n",
    "#                 results_dictionary.update(\n",
    "#                     {\"MRQAP_av_\" + aggr_pref + \"_\" + \"|\".join(multidim_group_i) + \"-\" + i:[] for i in onedim_groups_all}\n",
    "#                 )\n",
    "#                 results_dictionary.update(\n",
    "#                     {\"MRQAP_std_\" + aggr_pref + \"_\" + \"|\".join(multidim_group_i) + \"-\" + i:[] for i in onedim_groups_all}\n",
    "#                 )\n",
    "\n",
    "    ## Likelihood, AIC, BIC\n",
    "    results_dictionary.update({\"L_\"+i:[] for i in aggr_pref_combinations})\n",
    "    results_dictionary.update({\"AIC_\"+i:[] for i in aggr_pref_combinations})\n",
    "    results_dictionary.update({\"BIC_\"+i:[] for i in aggr_pref_combinations})\n",
    "    \n",
    "    return results_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b45ca5",
   "metadata": {},
   "source": [
    "### Functions to introduce data in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c62245da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_group_sizes(n,results_dictionary,multidim_groups,all_attributes_dict):\n",
    "    for col in multidim_groups:\n",
    "        if col in n.columns:\n",
    "            results_dictionary[\"N_\"+\"|\".join(col)].append(n[col][\"N\"])\n",
    "        else:\n",
    "            results_dictionary[\"N_\"+\"|\".join(col)].append(np.nan)\n",
    "    for d, attr_lst in all_attributes_dict.items():\n",
    "        aggr_counts = n.transpose().groupby(d).sum()\n",
    "        for i in attr_lst:\n",
    "            if i in aggr_counts[\"N\"]:\n",
    "                results_dictionary[\"N_\"+i].append(aggr_counts[\"N\"][i])\n",
    "            else:\n",
    "                results_dictionary[\"N_\"+i].append(np.nan)\n",
    "    return results_dictionary\n",
    "\n",
    "def save_multidim_preferences(\n",
    "    aggr_fun,\n",
    "    H,\n",
    "    multidim_groups,\n",
    "    results_dictionary\n",
    "    ):\n",
    "    for col1 in multidim_groups:\n",
    "        for col2 in multidim_groups:\n",
    "            ## To extract info from a pandas dataframe it goes like df[column][row] instead of [row][column] like an array\n",
    "            if col2 in H.columns and col1 in H.index:\n",
    "                results_dictionary[\"H_\" + aggr_fun + \"_\" + \"|\".join(col1) + \"-\" + \"|\".join(col2)].append(H[col2][col1])\n",
    "            else:\n",
    "                results_dictionary[\"H_\" + aggr_fun + \"_\" + \"|\".join(col1) + \"-\" + \"|\".join(col2)].append(np.nan)\n",
    "    return results_dictionary\n",
    "\n",
    "def save_likelihoods(\n",
    "    aggr_pref,\n",
    "    likel,\n",
    "    results_dictionary\n",
    "    ):\n",
    "    results_dictionary[\"L_\"+aggr_pref].append(likel[0])\n",
    "    results_dictionary[\"AIC_\"+aggr_pref].append(likel[1])\n",
    "    results_dictionary[\"BIC_\"+aggr_pref].append(likel[2])\n",
    "    return results_dictionary\n",
    "    \n",
    "def save_1d_simple_preferences(\n",
    "    aggr_fun,\n",
    "    dimensions_list,\n",
    "    h_est_simple,\n",
    "    results_dictionary,\n",
    "    all_attributes_dict,\n",
    "    name_prefix = \"h_\" ## h_norm_\n",
    "    ):\n",
    "    for i, d in enumerate(dimensions_list):\n",
    "        attr_lst_d = all_attributes_dict[d]\n",
    "        onedim_pairs_d = itertools.product(attr_lst_d,attr_lst_d) \n",
    "        for onedim_pair_i in onedim_pairs_d:\n",
    "            try:\n",
    "                h_est_simple[i][onedim_pair_i[1]][onedim_pair_i[0]]\n",
    "            except KeyError:\n",
    "                results_dictionary[name_prefix + aggr_fun + \"_1d-simple_\" + d + \"_\" + \"-\".join(onedim_pair_i)].append(np.nan)\n",
    "                continue\n",
    "            ## To extract info from a pandas dataframe it goes like df[column][row] instead of [row][column] like an array\n",
    "            results_dictionary[name_prefix + aggr_fun + \"_1d-simple_\" + d + \"_\" + \"-\".join(onedim_pair_i)].append(h_est_simple[i][onedim_pair_i[1]][onedim_pair_i[0]])\n",
    "    return results_dictionary\n",
    "\n",
    "def save_multi_1d_preferences(\n",
    "    dimensions_list,\n",
    "    h_est_multi_1d,\n",
    "    results_dictionary,\n",
    "    all_attributes_dict,\n",
    "    aggr_fun = \"and\"\n",
    "    ):\n",
    "    for i, d in enumerate(dimensions_list):\n",
    "        attr_list = all_attributes_dict[d]\n",
    "        ## Assert that there are some non-nan data there (and/or that I'm correctly aligning the matrices from h_est_multi_1d with my lists of attributes)\n",
    "        assert set(h_est_multi_1d[i].columns).intersection(set(attr_list))\n",
    "        assert set(h_est_multi_1d[i].index).intersection(set(multidim_groups))\n",
    "        for multi_group in multidim_groups:\n",
    "            for onedim_group in attr_list:\n",
    "                try:\n",
    "                    val = h_est_multi_1d[i][onedim_group][multi_group]\n",
    "                except KeyError:\n",
    "                    val = np.nan\n",
    "                results_dictionary[\"h_\" + aggr_fun + \"_multi-1d_\" + \"|\".join(multi_group) + \"-\" + onedim_group].append(val)\n",
    "    return results_dictionary\n",
    "\n",
    "def save_full_1d_preferences(\n",
    "    dimensions_list,\n",
    "    h_est_full_1d,\n",
    "    results_dictionary,\n",
    "    all_attributes_dict,\n",
    "    aggr_fun = \"and\"\n",
    "    ):\n",
    "    for i1, d1 in enumerate(dimensions_list):\n",
    "        for i2, d2 in enumerate(dimensions_list):\n",
    "            h_est_d1d2 = h_est_full_1d[(i1,i2)]\n",
    "            ## Assert that there are some non-nan data there (and/or that I'm correctly aligning the matrices from h_est_full_1d with my lists of attributes)\n",
    "            assert set(h_est_d1d2.columns).intersection(set(all_attributes_dict[d2]))\n",
    "            assert set(h_est_d1d2.index).intersection(set(all_attributes_dict[d1]))\n",
    "            for attr1 in all_attributes_dict[d1]:\n",
    "                for attr2 in all_attributes_dict[d2]:\n",
    "                    try:\n",
    "                        val = h_est_d1d2[attr2][attr1]\n",
    "                    except KeyError:\n",
    "                        val = np.nan\n",
    "                    results_dictionary[\"h_\" + aggr_fun + \"_1d-full_\" + attr1 + \"-\" + attr2].append(val)\n",
    "    return results_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de84026",
   "metadata": {},
   "source": [
    "### Main computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f0c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrqap_iter = 100\n",
    "data_path = \"../multidimensional_homophily/Datasets/AddHealth\" ## Change by the path to the AddHealth dataset\n",
    "results_path = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f100d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community #2: one school\n",
      "There is only one sex category or only one race category: we skip it\n",
      "Community #3: one school\n",
      "There are less than 100 students in this school: we skip it\n",
      "Community #4: 2 schools\n",
      "There is only one sex category or only one race category: we skip it\n",
      "Community #5: one school\n",
      "There is only one sex category or only one race category: we skip it\n",
      "Community #6: one school\n",
      "There is only one sex category or only one race category: we skip it\n",
      "Community #7: 2 schools\n",
      "There are 36 parameters\n",
      "----------------------\n",
      "Multidimensional\n",
      "----------------------\n",
      "Simple  and\n",
      "Likelihood maximization convergence result: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "----------------------\n",
      "Simple  or\n",
      "Likelihood maximization convergence result: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "----------------------\n",
      "Simple  mean\n",
      "Likelihood maximization convergence result: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "----------------------\n",
      "Multi->1D\n",
      "Likelihood maximization convergence result: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "(-750.0418505494131, 2286.083701098826, 4316.729586725329, 393)\n",
      "----------------------\n",
      "Full 1D\n"
     ]
    }
   ],
   "source": [
    "results_dictionary = init_results_dictionary(all_attributes_dict,aggr_pref_combinations)\n",
    "\n",
    "for school in range(1,8): ## For tests\n",
    "# for school in range(1,85): ## Full computation\n",
    "    t0 = time.time()\n",
    "    if school in bad_comms:\n",
    "        continue\n",
    "#     try:\n",
    "    nodes_list,edges_list = data_loader.load_AddHealth(school, \n",
    "                                                       th_sex = 20, \n",
    "                                                       th_race= 20, \n",
    "                                                       th_grade= 20, \n",
    "                                                       school= None, \n",
    "                                                       remove_unreported=True,\n",
    "                                                       data_path = data_path\n",
    "                                                      )\n",
    "#     except: print('Error')\n",
    "    if len(nodes_list['sex'].cat.categories)==1 or len(nodes_list['race'].cat.categories)==1:\n",
    "        print('There is only one sex category or only one race category: we skip it')\n",
    "    elif nodes_list.shape[0]<100:\n",
    "        print('There are less than 100 students in this school: we skip it')\n",
    "    else:\n",
    "        ## Store school number and size\n",
    "        results_dictionary[\"school\"].append(school)\n",
    "        results_dictionary[\"N\"].append(nodes_list.shape[0])\n",
    "        \n",
    "        ## Compute group sizes\n",
    "        n0,counts0 = aux_functions.get_n_and_counts(nodes_list,edges_list,dimensions_list)\n",
    "        ## To ensure CONSISTENT ORDERING\n",
    "        ## First extract only the columns that exist in the dataframe, otherwise we \n",
    "        ## run into problems because, even if we drop NaNs, the resulting dataframe, \n",
    "        ## internally, thinks that there are more nonempty columns than there really are\n",
    "        new_cols = [i for i in multidim_groups if i in n0.columns] \n",
    "        n = n0.reindex(columns=new_cols)#.dropna(how=\"all\",axis=1) ## If we use this instead of \"new_cols\" we end up with empty columns that change n.columns.levshape which is used later for many computations (g_vec=n.columns.levshape)\n",
    "        \n",
    "        new_index = [i for i in multidim_groups if i in counts0.index]\n",
    "        new_cols = [i for i in multidim_groups if i in counts0.columns]\n",
    "        counts = counts0.reindex(index=new_index,columns=new_cols       ## Change order of columns and rows\n",
    "                                )#.dropna(how=\"all\",axis=0).dropna(how=\"all\",axis=1) ## Remove empty rows and columns\n",
    "        \n",
    "        ## Compute 1D group sizes and inter-group links\n",
    "        att_pop0 = [n.T.groupby(level=i, sort=False).sum() for i in range(num_dimensions)]\n",
    "        att_counts0 = [counts.T.groupby(level=i,sort=False).sum().T.groupby(level=i, sort=False).sum() for i in range(num_dimensions)]\n",
    "        ## To ensure consistent ordering\n",
    "        att_counts = []\n",
    "        for i, cnts in enumerate(att_counts0):\n",
    "            assert set(all_attributes_dict[dimensions_list[i]]).intersection(set(cnts.index))\n",
    "            assert set(all_attributes_dict[dimensions_list[i]]).intersection(set(cnts.columns))\n",
    "            new_index = [attr_i for attr_i in all_attributes_dict[dimensions_list[i]] if attr_i in cnts.index]\n",
    "            new_cols = [attr_i for attr_i in all_attributes_dict[dimensions_list[i]] if attr_i in cnts.columns]\n",
    "            att_counts.append( cnts.reindex(index=new_index, columns=new_cols) )\n",
    "        att_pop = []\n",
    "        for i, cnts in enumerate(att_pop0):\n",
    "            assert set(all_attributes_dict[dimensions_list[i]]).intersection(set(cnts.index))\n",
    "            new_index = [attr_i for attr_i in all_attributes_dict[dimensions_list[i]] if attr_i in cnts.index]\n",
    "            att_counts.append( cnts.reindex(index=new_index) )\n",
    "#         att_counts = [ cnts.reindex(index=all_attributes_dict[dimensions_list[i]],columns=all_attributes_dict[dimensions_list[i]]\n",
    "#                                    ).dropna(how=\"all\",axis=1).dropna(how=\"all\",axis=0) \n",
    "#                       for i,cnts in enumerate(att_counts0)]\n",
    "#         att_pop = [ cnts.reindex(index=all_attributes_dict[dimensions_list[i]]\n",
    "#                                    ).dropna(how=\"all\",axis=0) \n",
    "#                       for i,cnts in enumerate(att_pop0)]\n",
    "        \n",
    "        print(f'There are {n.shape[1]} parameters')\n",
    "        results_dictionary = save_group_sizes(n,results_dictionary,multidim_groups,all_attributes_dict)\n",
    "\n",
    "        g_vec = n.columns.levshape\n",
    "\n",
    "        ################################################\n",
    "        ## Multidimensional\n",
    "        print (\"----------------------\\nMultidimensional\")\n",
    "        H_est_multi, _, num_params = inference.estimate_H(n,counts,type_p='multidimensional',print_convergence=True)\n",
    "        likel_multi = inference.compute_likel(n,counts,H_est_multi,k=num_params,print_values=False)\n",
    "        \n",
    "        results_dictionary = save_multidim_preferences(\n",
    "            \"multi-full\",\n",
    "            H_est_multi,\n",
    "            multidim_groups,\n",
    "            results_dictionary\n",
    "            )\n",
    "        results_dictionary = save_likelihoods(\n",
    "            \"multi-full\",\n",
    "            likel_multi,\n",
    "            results_dictionary\n",
    "            )\n",
    "        \n",
    "        ################################################\n",
    "        ## Simple\n",
    "        for aggr_fun in aggregation_functions:\n",
    "            print (\"----------------------\\nSimple \", aggr_fun)\n",
    "            H_est_simple, x_est, num_params = inference.estimate_H(n,counts,type_p=aggr_fun,print_convergence=True)\n",
    "            num_free_params = aux_functions.product_mean_free_params(g_vec)\n",
    "            likel_simple = inference.compute_likel(n,counts,H_est_simple,k=num_free_params,print_values=False)\n",
    "            \n",
    "            ## Store \"and\" result for later for MRQAP\n",
    "            if aggr_fun == \"and\":\n",
    "                x_est_and = copy.deepcopy(x_est)\n",
    "\n",
    "            results_dictionary = save_multidim_preferences(\n",
    "                aggr_fun + \"_1d-simple\",\n",
    "                H_est_simple,\n",
    "                multidim_groups,\n",
    "                results_dictionary\n",
    "                )\n",
    "            results_dictionary = save_likelihoods(\n",
    "                aggr_fun + \"_1d-simple\",\n",
    "                likel_simple,\n",
    "                results_dictionary\n",
    "                )\n",
    "            \n",
    "            ## Extract and save 1D preferences\n",
    "            h_est_mtrx = aux_functions.vec_to_mat_list(x_est,g_vec)\n",
    "            h_est_simple = []\n",
    "            h_est_norm = []\n",
    "            for i in range(num_dimensions):\n",
    "                h_est_simple.append(pd.DataFrame(h_est_mtrx[i],index=att_counts[i].index,columns=att_counts[i].columns))\n",
    "                h_est_norm.append(h_est_simple[i].div(h_est_simple[i].to_numpy().diagonal(), axis=0))\n",
    "            save_1d_simple_preferences(\n",
    "                aggr_fun,\n",
    "                dimensions_list,\n",
    "                h_est_simple,\n",
    "                results_dictionary,\n",
    "                all_attributes_dict,\n",
    "                name_prefix = \"h_\" ## h_norm\n",
    "                )\n",
    "            save_1d_simple_preferences(\n",
    "                aggr_fun,\n",
    "                dimensions_list,\n",
    "                h_est_norm,\n",
    "                results_dictionary,\n",
    "                all_attributes_dict,\n",
    "                name_prefix = \"h_norm_\"\n",
    "                )\n",
    "        \n",
    "        ################################################\n",
    "        ## Multi -> 1D\n",
    "        print (\"----------------------\\nMulti->1D\")\n",
    "        H_est_multi_1d, x_est, num_params = inference.estimate_H(n,counts,\n",
    "                                                                 type_p=\"product_weights\",\n",
    "                                                                 print_convergence=True,\n",
    "                                                                opt_options = {'ftol':1e-10,'gtol':1e-10,'maxfun':1000000})\n",
    "        num_free_params = aux_functions.multi_1d_free_params(g_vec)\n",
    "        likel_multi_1d = inference.compute_likel(n,counts,H_est_multi_1d,k=num_free_params,print_values=False)\n",
    "        print(likel_multi_1d)\n",
    "        \n",
    "        results_dictionary = save_multidim_preferences(\n",
    "                \"and_multi-1d\",\n",
    "                H_est_multi_1d,\n",
    "                multidim_groups,\n",
    "                results_dictionary\n",
    "                )\n",
    "        results_dictionary = save_likelihoods(\n",
    "                \"and_multi-1d\",\n",
    "                likel_multi_1d,\n",
    "                results_dictionary\n",
    "                )\n",
    "        \n",
    "        ## Extract and save multi->1D preferences\n",
    "        h_est_mtrx = aux_functions.vec_to_weights_matrix(x_est,g_vec)\n",
    "        h_est_multi_1d = []\n",
    "        for d in range(num_dimensions):\n",
    "            h_est_multi_1d.append(pd.DataFrame(h_est_mtrx[d],index=H_est_multi_1d.index,columns=att_counts[d].columns))\n",
    "\n",
    "        results_dictionary = save_multi_1d_preferences(\n",
    "            dimensions_list,\n",
    "            h_est_multi_1d,\n",
    "            results_dictionary,\n",
    "            all_attributes_dict,\n",
    "            aggr_fun = \"and\"\n",
    "            )\n",
    "        \n",
    "        ################################################\n",
    "        ## Full 1D\n",
    "        print (\"----------------------\\nFull 1D\")\n",
    "        H_est_full_1d, x_est, num_params = inference.estimate_H(n,counts,\n",
    "                                                                type_p=\"product_cross_dimensional\",\n",
    "                                                                print_convergence=True,\n",
    "                                                               opt_options = {'ftol':1e-10,'gtol':1e-10,'maxfun':1000000})\n",
    "        num_free_params = aux_functions.full_1d_free_params(g_vec)\n",
    "        likel_full_1d = inference.compute_likel(n,counts,H_est_full_1d,k=num_free_params,print_values=False)\n",
    "        print(likel_full_1d)\n",
    "        \n",
    "        results_dictionary = save_multidim_preferences(\n",
    "                \"and_1d-full\",\n",
    "                H_est_full_1d,\n",
    "                multidim_groups,\n",
    "                results_dictionary\n",
    "                )\n",
    "        results_dictionary = save_likelihoods(\n",
    "                \"and_1d-full\",\n",
    "                likel_full_1d,\n",
    "                results_dictionary\n",
    "                )\n",
    "        \n",
    "        ## Extract and save full 1D preferences\n",
    "        h_est_mtrx = aux_functions.vec_to_mat_dict_cross_one_dimensional(x_est,g_vec)\n",
    "        h_est_full_1d = {}\n",
    "        for d1 in range(num_dimensions):\n",
    "            for d2 in range(num_dimensions):\n",
    "                h_est_full_1d[(d1,d2)] = pd.DataFrame(h_est_mtrx[(d1,d2)],index=att_counts[d1].index,columns=att_counts[d2].columns)\n",
    "            \n",
    "        results_dictionary = save_full_1d_preferences(\n",
    "            dimensions_list,\n",
    "            h_est_full_1d,\n",
    "            results_dictionary,\n",
    "            all_attributes_dict,\n",
    "            aggr_fun = \"and\"\n",
    "            )\n",
    "        \n",
    "        ################################################\n",
    "        ## MRQAP \"and simple\"\n",
    "        print (\"MRQAP\")\n",
    "        \n",
    "#         num_params = np.sum(g_vec)**2\n",
    "        num_params = len(x_est_and)\n",
    "        X_rnd = np.zeros((mrqap_iter,num_params))\n",
    "        X_rnd_norm = np.zeros((mrqap_iter,num_params))\n",
    "        \n",
    "        ## Initialize node list copy to randomize\n",
    "        nodes_list_rnd = copy.deepcopy(nodes_list)\n",
    "        \n",
    "        for it in tqdm(range(mrqap_iter)):\n",
    "            \n",
    "            ## Randomize nodes order\n",
    "            nodes_list_rnd[:] = nodes_list.sample(frac=1,replace=False).values\n",
    "            \n",
    "            ## Count inter-group links\n",
    "            n_rnd0,counts_rnd0 = aux_functions.get_n_and_counts(nodes_list_rnd,edges_list,dimensions_list)\n",
    "            ## To ensure consistent ordering\n",
    "            new_cols = [i for i in multidim_groups if i in n_rnd0.columns] \n",
    "            n_rnd = n_rnd0.reindex(columns=new_cols)\n",
    "            \n",
    "            new_index = [i for i in multidim_groups if i in counts_rnd0.index]\n",
    "            new_cols = [i for i in multidim_groups if i in counts_rnd0.columns]\n",
    "            counts_rnd = counts_rnd0.reindex(index=new_index,columns=new_cols)\n",
    "    \n",
    "            ## Infer preferences\n",
    "            _, x_rnd, _ = inference.estimate_H(n_rnd,counts_rnd,type_p=\"and\",print_convergence=True)\n",
    "            X_rnd[it,:] = x_rnd\n",
    "            \n",
    "            ## Normalize x_rnd\n",
    "            h_mtrx_rnd = aux_functions.vec_to_mat_list(x_rnd,g_vec)\n",
    "            h_mtrx_rnd_norm = [np.divide(i.T, np.diag(i)).T for i in h_mtrx_rnd]\n",
    "            x_rnd_norm = np.array(list(itertools.chain.from_iterable([i.ravel() for i in h_mtrx_rnd_norm])))\n",
    "            X_rnd_norm[it,:] = x_rnd_norm\n",
    "            \n",
    "        X_av = np.mean(X_rnd,axis=0)\n",
    "        X_std = np.std(X_rnd,axis=0)\n",
    "        X_norm_av = np.mean(X_rnd_norm,axis=0)\n",
    "        X_norm_std = np.std(X_rnd_norm,axis=0)\n",
    "        \n",
    "        ## Normalized version of empirical preferences\n",
    "        h_mtrx_est_and = aux_functions.vec_to_mat_list(x_est_and,g_vec)\n",
    "        h_mtrx_est_and_norm = [np.divide(i.T, np.diag(i)).T for i in h_mtrx_est_and]\n",
    "        x_est_and_norm = np.array(list(itertools.chain.from_iterable([i.ravel() for i in h_mtrx_est_and_norm])))\n",
    "\n",
    "        ## Compute p-value\n",
    "        X_pval_oneside = np.sum(np.less_equal(X_rnd_norm,x_est_and_norm),axis=0) / X_rnd.shape[0] ## Proportion of iterations where preference is less than empirical\n",
    "        assert np.all(X_pval_oneside>=0) and np.all(X_pval_oneside<=1) ## To avoid further mistakes\n",
    "        X_pval_twoside = copy.deepcopy(X_pval_oneside)\n",
    "        X_pval_twoside[X_pval_twoside>0.5] = 1-X_pval_twoside[X_pval_twoside>0.5]\n",
    "        ## Careful, this is not the two-sided, which would be 2 times that value if the distribution is symmetric\n",
    "        \n",
    "        data_and_labels = [\n",
    "            (X_av,\"MRQAP_av_h_\"),\n",
    "            (X_std,\"MRQAP_std_h_\"),\n",
    "            (X_norm_av,\"MRQAP_av_h_norm_\"),\n",
    "            (X_norm_std,\"MRQAP_std_h_norm_\"),\n",
    "            (X_pval_oneside,\"MRQAP_pval1s_h_\"),\n",
    "            (X_pval_twoside,\"MRQAP_pval2s_h_\")\n",
    "            ]\n",
    "        \n",
    "        ## Extract and save 1D average preferences\n",
    "        att_counts_rnd0 = [counts_rnd.T.groupby(level=i,sort=False).sum().T.groupby(level=i, sort=False).sum() for i in range(num_dimensions)]\n",
    "        ## To ensure consistent ordering\n",
    "        att_counts_rnd = []\n",
    "        for i, cnts in enumerate(att_counts_rnd0):\n",
    "            assert set(all_attributes_dict[dimensions_list[i]]).intersection(set(cnts.index))\n",
    "            assert set(all_attributes_dict[dimensions_list[i]]).intersection(set(cnts.columns))\n",
    "            new_index = [attr_i for attr_i in all_attributes_dict[dimensions_list[i]] if attr_i in cnts.index]\n",
    "            new_cols = [attr_i for attr_i in all_attributes_dict[dimensions_list[i]] if attr_i in cnts.columns]\n",
    "            att_counts_rnd.append( cnts.reindex(index=new_index, columns=new_cols) )\n",
    "        \n",
    "        for x_est, name_prefix in data_and_labels:\n",
    "            h_est_mtrx = aux_functions.vec_to_mat_list(x_est,g_vec)\n",
    "            h_est_simple = []\n",
    "            for i in range(num_dimensions):\n",
    "                h_est_simple.append(pd.DataFrame(h_est_mtrx[i],index=att_counts_rnd[i].index,columns=att_counts_rnd[i].columns))\n",
    "\n",
    "            save_1d_simple_preferences(\n",
    "                    \"and\",\n",
    "                    dimensions_list,\n",
    "                    h_est_simple,\n",
    "                    results_dictionary,\n",
    "                    all_attributes_dict,\n",
    "                    name_prefix = name_prefix\n",
    "                    )\n",
    "    \n",
    "        ## Save each iteration to ensure that even if there is an error we get some data\n",
    "        results_df = pd.DataFrame(results_dictionary)\n",
    "        results_df.to_csv(results_path+\"/paper_results.csv\")\n",
    "        ## v2 -- introduced num_free_params, which gives the correct AIC and BIC for simple, multi-1D, and full-1d\n",
    "        ## v3 -- corrected MRQAP incorrect normalization X_rnd.shape[0] instead of original X_rnd.shape[1]; I also changed comparison to X_rnd_norm, because due to unidentifiability of product, comparing unnormalized preferences may be unreliable\n",
    "        print (\"Time for full iteration: \", time.time()-t0)\n",
    "        print (\"++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d7101a-9069-4103-bf29-918b2f19186d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48a99966-825a-49c2-9814-40d709d24589",
   "metadata": {},
   "source": [
    "### Checking consistency of old and new results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd149e81-24c1-4b57-861e-01a2d11e7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_results = pd.read_csv(\"preference_results_v4_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9106135-1ce5-41ae-8dd2-260a49038a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df[\"school\"]==7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a189da-7c69-410b-8308-1b0c99eec6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_results[old_results[\"school\"]==7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad0514-1253-40b1-86c4-4be14ef44ca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## New results are almost the exact same but for AIC and BIC of full multidimensional model because now we subtract the \n",
    "## number of NaN elements in the multidimensional matrix to be a bit more conservative\n",
    "\n",
    "## Some of the estimated latent preferences can also be different because the model is not fully specified - in the\n",
    "## paper we explain that to deal with that issue we normalize preferences. After adequate normalization (e.g. row-wise)\n",
    "## preferences are the same between different runs.\n",
    "\n",
    "xx = []\n",
    "yy = []\n",
    "for col in results_df.columns:\n",
    "    print (col, old_results[old_results[\"school\"]==7][col].values[0], results_df[col].values[0])\n",
    "    xx.append(old_results[old_results[\"school\"]==7][col].values[0])\n",
    "    yy.append(results_df[col].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d81fd-3a08-4ed6-92a9-9b080873f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "plt.plot(xx,yy,\"o\")\n",
    "plt.plot([min(xx),max(xx)],[min(xx),max(xx)],\"-k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f50d21c-0c70-447a-9965-9e02fc3981f6",
   "metadata": {},
   "source": [
    "#### Example of normalization with multi->1D preference structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb5339-f590-4e40-bfa1-82dfa3d89312",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "\n",
    "for col in results_df.columns:\n",
    "    if col.startswith(\"h_and_multi-1d_7th|Hispanic|Female\") and col.endswith(\"ale\"):\n",
    "        print (col, old_results[old_results[\"school\"]==7][col].values[0], results_df[col].values[0])\n",
    "        xx.append(old_results[old_results[\"school\"]==7][col].values[0])\n",
    "        yy.append(results_df[col].values[0])\n",
    "        \n",
    "xx = np.array(xx)\n",
    "yy = np.array(yy)\n",
    "\n",
    "print (\"Same values after normalization\", xx/xx[~np.isnan(xx)].sum(), yy/yy[~np.isnan(xx)].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6571b-0730-42c2-b77f-bf7765acf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "\n",
    "for col in results_df.columns:\n",
    "    if col.startswith(\"h_and_multi-1d_7th|Hispanic|Female\") and col.endswith(\"th\"):\n",
    "        print (col, old_results[old_results[\"school\"]==7][col].values[0], results_df[col].values[0])\n",
    "        xx.append(old_results[old_results[\"school\"]==7][col].values[0])\n",
    "        yy.append(results_df[col].values[0])\n",
    "        \n",
    "xx = np.array(xx)\n",
    "yy = np.array(yy)\n",
    "\n",
    "print (\"Same values after normalization\", \"\\n\",xx/xx[~np.isnan(xx)].sum(), \"\\n\",yy/yy[~np.isnan(xx)].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341adcc-6dc8-41c5-ac13-6cfaff8e3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "\n",
    "for col in results_df.columns:\n",
    "    if col.startswith(\"h_and_multi-1d_7th|Hispanic|Female\") and not col.endswith(\"th\") and not col.endswith(\"ale\"):\n",
    "        print (col, old_results[old_results[\"school\"]==7][col].values[0], results_df[col].values[0])\n",
    "        xx.append(old_results[old_results[\"school\"]==7][col].values[0])\n",
    "        yy.append(results_df[col].values[0])\n",
    "        \n",
    "xx = np.array(xx)\n",
    "yy = np.array(yy)\n",
    "\n",
    "print (\"Same values after normalization\", \"\\n\",xx/xx[~np.isnan(xx)].sum(), \"\\n\",yy/yy[~np.isnan(xx)].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e46e55-cf79-43c2-99b4-fd4c02a6a1b3",
   "metadata": {},
   "source": [
    "#### Check MRQAP values\n",
    "Some values can be over 1 because in the normalization we divide by the diagonal, so it is natural that the average of MRQAP, after normalization, is around 1 (all values should be quite neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4015f5f3-2c9e-4f67-9241-59a79b84a9a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "\n",
    "for col in results_df.columns:\n",
    "    # if \"MRQAP\" in col and \"norm\" in col and \"av\" in col:\n",
    "    if \"MRQAP\" in col:\n",
    "        print (col, old_results[old_results[\"school\"]==7][col].values[0], results_df[col].values[0])\n",
    "        xx.append(old_results[old_results[\"school\"]==7][col].values[0])\n",
    "        yy.append(results_df[col].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb4435-37f0-4dec-a6e2-e50c0c18bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "plt.plot(xx,yy,\"o\")\n",
    "plt.plot([0,1.2],[0,1.2],\"-k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2766e-7642-4236-9e6c-4f207a7213fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
